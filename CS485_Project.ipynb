{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS485_Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python392jvsc74a57bd08fe077956e84e6b59f116a7e9738aabbdf5e9bca6b48786bc1c1513955207840",
      "display_name": "Python 3.9.2 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.2"
    },
    "metadata": {
      "interpreter": {
        "hash": "8fe077956e84e6b59f116a7e9738aabbdf5e9bca6b48786bc1c1513955207840"
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ-thlkU_jMa"
      },
      "source": [
        "# 1. Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iASoq88Q_PJB",
        "outputId": "a1607133-9ff1-48ac-88d8-155a0b62c51b"
      },
      "source": [
        "# Install Dependencies\n",
        "!pip install pandas sklearn numerapi keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_mY2fHK_YrS"
      },
      "source": [
        "# Import Dependencies\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numerapi, time, warnings, itertools\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LambdaCallback, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation, LSTM, Bidirectional\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from sklearn.model_selection import train_test_split, GroupKFold, GridSearchCV\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import preprocessing\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#pd.set_option('chained_assignment', None)\n",
        "\n",
        "# ignore warning messages\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Background Functions"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_group_stats(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        for group in [\"intelligence\", \"wisdom\", \"charisma\", \"dexterity\", \"strength\", \"constitution\"]:\n",
        "            cols = [col for col in df.columns if group in col]\n",
        "            df[f\"feature_{group}_mean\"] = df[cols].mean(axis=1)\n",
        "            df[f\"feature_{group}_std\"] = df[cols].std(axis=1)\n",
        "            df[f\"feature_{group}_skew\"] = df[cols].skew(axis=1)\n",
        "        return df\n",
        "\n",
        "\n",
        "def sharpe_ratio(corrs: pd.Series) -> np.float32:\n",
        "        \"\"\"\n",
        "        Calculate the Sharpe ratio for Numerai by using grouped per-era data\n",
        "\n",
        "        :param corrs: A Pandas Series containing the Spearman correlations for each era\n",
        "        :return: A float denoting the Sharpe ratio of your predictions.\n",
        "        \"\"\"\n",
        "        return corrs.mean() / corrs.std()\n",
        "\n",
        "\n",
        "def evaluate(df: pd.DataFrame) -> tuple:\n",
        "        \"\"\"\n",
        "        Evaluate and display relevant metrics for Numerai \n",
        "\n",
        "        :param df: A Pandas DataFrame containing the columns \"era\", \"target\" and a column for predictions\n",
        "        :param pred_col: The column where the predictions are stored\n",
        "        :return: A tuple of float containing the metrics\n",
        "        \"\"\"\n",
        "        def _score(sub_df: pd.DataFrame) -> np.float32:\n",
        "            \"\"\"Calculates Spearman correlation\"\"\"\n",
        "            return spearmanr(sub_df[\"target\"], sub_df[\"prediction\"])[0]\n",
        "\n",
        "        # Calculate metrics\n",
        "        corrs = df.groupby(\"era\").apply(_score)\n",
        "        print(corrs)\n",
        "        payout_raw = (corrs / 0.2).clip(-1, 1)\n",
        "        spearman = round(corrs.mean(), 4)\n",
        "\n",
        "        payout = round(payout_raw.mean(), 4)\n",
        "        numerai_sharpe = round(sharpe_ratio(corrs), 4)\n",
        "        mae = mean_absolute_error(df[\"target\"], df[\"prediction\"]).round(4)\n",
        "\n",
        "        # Display metrics\n",
        "        print(f\"Spearman Correlation: {spearman}\")\n",
        "        print(f\"Average Payout: {payout}\")\n",
        "        print(f\"Sharpe Ratio: {numerai_sharpe}\")\n",
        "        print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "        return spearman, payout, numerai_sharpe, mae"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUk0RCWXWrUP"
      },
      "source": [
        "# 2. Numerai Tournament API setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnuUeSAiWvPf"
      },
      "source": [
        "# Get your API keys and model_id from https://numer.ai/submit\n",
        "public_id = \"INSERT PUBLIC ID\"\n",
        "secret_key = \"INSERT SECRET KEY\"\n",
        "model_id = \"INSERT MODEL ID\"\n",
        "napi = numerapi.NumerAPI(public_id=public_id, secret_key=secret_key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUEh-z4D_hBK"
      },
      "source": [
        "# 3. Download Data Sets\n",
        "\n",
        "### Datasets \n",
        "*   `trainingData` is used to train the model\n",
        "*   `tournamentData` is used to evaluate the model\n",
        "\n",
        "### Column descriptions\n",
        "*   id: a randomized id that corresponds to a stock \n",
        "*   era: a period of time\n",
        "*   data_type: either `train`, `validation`, `test`, or `live` \n",
        "*   feature_*: abstract financial features of the stock \n",
        "*   target: abstract measure of stock performance\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_WfC75D_sFk"
      },
      "source": [
        "# Download Training Data From Numerai\n",
        "start = time.time()\n",
        "print(f\"[{time.asctime()}] Downloading the lastest training data set. Current round is: {numerapi.NumerAPI(verbosity='info').get_current_round()}...\\n\")\n",
        "trainingData = pd.read_csv(\"https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_training_data.csv.xz\", header=0)\n",
        "end = time.time()\n",
        "print(f\"[{time.asctime()}] Training dataset has been loaded. It took {end - start:0.2f} seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download Tournament Data From Numerai\n",
        "start = time.time()\n",
        "print(f\"[{time.asctime()}] Downloading the lastest tournament data set. Current round is: {numerapi.NumerAPI(verbosity='info').get_current_round()}...\\n\")\n",
        "tournamentData = pd.read_csv(\"https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_tournament_data.csv.xz\", header=0)\n",
        "end = time.time()\n",
        "print(f\"[{time.asctime()}] Tournament dataset has been loaded. It took {end - start:0.2f} seconds\")"
      ]
    },
    {
      "source": [
        "# 4. Explore The Dataset"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "AxL1hBnGD3K7",
        "outputId": "0427332d-a18f-457a-87a5-ea9f2a0fc342"
      },
      "source": [
        "# Print Training Data\n",
        "trainingData.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find Unique Items Within The Training Data\n",
        "print(f'{len(trainingData.era.unique())} UNIQUE ERAS: {trainingData.era.unique()}\\n')\n",
        "print(f'{len(trainingData.data_type.unique())} UNIQUE DATA TYPE: {trainingData.data_type.unique()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print Tournament Data\n",
        "tournamentData.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find Unique Items Within Tournament Data\n",
        "print(f'{len(tournamentData.data_type.unique())} UNIQUE DATA TYPE: {tournamentData.data_type.unique()}\\n')\n",
        "print(f'{len(tournamentData.era.unique())} UNIQUE ERAS: {tournamentData.era.unique()}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5Lom0GiDRqb"
      },
      "source": [
        "# Select Validation Data Out of Tournament Dataset\n",
        "validationData = tournamentData[tournamentData.data_type=='validation']\n",
        "\n",
        "# Select Test Data Out of Tournament Dataset\n",
        "testData = tournamentData[tournamentData.data_type=='test']\n",
        "\n",
        "# Select Live Data Out of Tournament Dataset\n",
        "liveData = tournamentData[tournamentData.data_type=='live']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"UNIQUE TRAINING TARGETS: {trainingData.target[trainingData.data_type=='train'].unique()}\")\n",
        "print(f\"UNIQUE VALIDATION TARGETS: {tournamentData.target[tournamentData.data_type=='validation'].unique()}\")\n",
        "print(f\"UNIQUE TEST TARGETS: {tournamentData.target[tournamentData.data_type=='test'].unique()}\")\n",
        "print(f\"UNIQUE LIVE TARGETS: {tournamentData.target[tournamentData.data_type=='live'].unique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot heatmap of feature correlation\n",
        "plt.figure(figsize=(15,15))\n",
        "sns.heatmap(trainingData.corr())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract era numbers\n",
        "trainingData[\"erano\"] = trainingData.era.str.slice(3).astype(int)\n",
        "plt.figure(figsize=[14, 6])\n",
        "trainingData.groupby(trainingData['erano'])[\"target\"].size().plot(title=\"Era sizes\", figsize=(14, 8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feats = [f for f in trainingData.columns if \"feature\" in f]\n",
        "plt.figure(figsize=(15, 5))\n",
        "sns.histplot(pd.DataFrame(trainingData[feats].std()),bins=100)\n",
        "plt.legend([\"Train\"], fontsize=20)\n",
        "plt.title(\"Standard deviations over training features in the data\", weight='bold', fontsize=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feats = [f for f in trainingData.columns if \"feature\" in f]\n",
        "plt.figure(figsize=(15, 5))\n",
        "sns.histplot(pd.DataFrame(validationData[feats].std()),bins=100)\n",
        "plt.legend([\"Val\"], fontsize=20)\n",
        "plt.title(\"Standard deviations over validation features in the data\", weight='bold', fontsize=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feats = [f for f in trainingData.columns if \"feature\" in f]\n",
        "plt.figure(figsize=(15, 5))\n",
        "sns.histplot(pd.DataFrame(testData[feats].std()), bins=100)\n",
        "plt.legend([\"Test\"], fontsize=20)\n",
        "plt.title(\"Standard deviations over test features in the data\", weight='bold', fontsize=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feats = [f for f in trainingData.columns if \"feature\" in f]\n",
        "plt.figure(figsize=(15, 5))\n",
        "sns.distplot(pd.DataFrame(trainingData[feats].std()),bins=100)\n",
        "sns.distplot(pd.DataFrame(validationData[feats].std()),bins=100)\n",
        "sns.distplot(pd.DataFrame(testData[feats].std()), bins=100)\n",
        "plt.legend([\"Train\", \"Val\", \"Test\"], fontsize=20)\n",
        "plt.title(\"Standard deviations over all features in the data\", weight='bold', fontsize=20)"
      ]
    },
    {
      "source": [
        "# 5. Feature Engineering"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Feature Correlation With Target Based On Era\n",
        "\n",
        "# Extract Unique Eras From Training Data\n",
        "eras = list(trainingData.era.unique())\n",
        "eraList = []\n",
        "for era in eras:\n",
        "    eraData = trainingData[trainingData.era==era]\n",
        "\n",
        "    # Calculate Correlations With Target\n",
        "    eraCorr = eraData.corr()\n",
        "    corrWithTarget = eraCorr[\"target\"].T.apply(abs).sort_values(ascending=False)\n",
        "\n",
        "    # Select Features With Highest Correlation To The Target Variable\n",
        "    features = corrWithTarget[:20]\n",
        "    features.drop(\"target\", inplace=True)\n",
        "\n",
        "    featureList = features.tolist()\n",
        "    eraList.append(featureList)\n",
        "\n",
        "    # # Write To A File\n",
        "    with open(f\"Correlations Round{numerapi.NumerAPI(verbosity='info').get_current_round()}.txt\",'a') as f:\n",
        "        f.write(f\"Top 10 Features in {era} according to correlation with target:\\n\")\n",
        "        f.write(f'{features[:10]}\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eraCorrList, topEras = [], []\n",
        "\n",
        "for (era, corrs) in zip(eras, eraList):\n",
        "    # Find Correlation Average Based On Era\n",
        "    corrTot = 0\n",
        "    for corr in corrs:\n",
        "        corrTot += corr\n",
        "    corrAVG = corrTot / len(corrs)\n",
        "    eraCorrList.append([era, corrAVG])\n",
        "\n",
        "# Sort Era Correlation List By Correlation Average\n",
        "eraCorrList.sort(key=lambda eraCorrList: eraCorrList[1], reverse=True)\n",
        "\n",
        "# Select The Top Correlated Eras\n",
        "for i in range(len(eraCorrList)):\n",
        "    if i == 20:\n",
        "        break\n",
        "    topEras.append(eraCorrList[i][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create New Training Data Set (WIP)\n",
        "headers = [h for h in trainingData.columns]\n",
        "dataFrameList = [pd.DataFrame(columns=headers)]\n",
        "for eras in topEras:\n",
        "    df = trainingData[trainingData.era==eras]\n",
        "    dataFrameList.append(df)\n",
        "# eras = [era for era in topEras]\n",
        "# print(f\"[{time.asctime()}] Creating new training data set based on top correlated features.\")\n",
        "# trainingDataEng = pd.concat([trainingDataEng,trainingData[trainingData.era==eras]],axis=1)\n",
        "# print(f\"[{time.asctime()}] Finished creating new training data set based on top correlated features.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interactions = preprocessing.PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
        "\n",
        "interactions.fit(trainingData[featureList], trainingData[\"target\"])\n",
        "\n",
        "X_train_interact = pd.DataFrame(interactions.transform(trainingData[featureList]))\n",
        "\n",
        "train=pd.concat([trainingData,X_train_interact],axis=1)\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select Features From Training Data\n",
        "#trainingDataX = to_categorical(trainingDataEng[trainingDataEng.columns[trainingDataEng.columns.str.startswith('feature')]])\n",
        "trainingDataX = to_categorical(trainingData[trainingData.columns[trainingData.columns.str.startswith('feature')]])\n",
        "\n",
        "# Select Targets From Training Data\n",
        "#trainingDataY = trainingData[trainingData.columns[trainingData.columns.str.startswith('target')]]\n",
        "trainingDataY = trainingData[trainingData.columns[trainingData.columns.str.startswith('target')]]\n",
        "# Converrt to numpy arrays\n",
        "trainingDataX = np.array(trainingDataX)\n",
        "trainingDataY = np.array(trainingDataY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split Up Data\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(trainingDataX, trainingDataY, test_size = 0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Linear data split\n",
        "validationDataX = validationData[validationData.columns[validationData.columns.str.startswith('feature')]]\n",
        "validationDataY = validationData[validationData.columns[validationData.columns.str.startswith('target')]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#RNN \n",
        "xTrain = xTrain.reshape(-1, 1, 310)\n",
        "xTest  = xTest.reshape(-1, 1, 310)\n",
        "yTrain = yTrain.reshape(-1, 1, 1)\n",
        "yTest = yTest.reshape(-1, 1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsow0byDDidK"
      },
      "source": [
        "# 6. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mason RNN\n",
        "model = Sequential()\n",
        "model.add(LSTM(128,return_sequences=True, input_shape=(xTrain.shape[1],xTrain.shape[2])))\n",
        "model.add(Bidirectional(LSTM(128,dropout=0.2,return_sequences=True,recurrent_dropout=0.2)))\n",
        "model.add(Bidirectional(LSTM(128,return_sequences=True,dropout=0.2,recurrent_dropout=0.2)))\n",
        "model.add(Bidirectional(LSTM(128,return_sequences=True,dropout=0.2,recurrent_dropout=0.2)))\n",
        "model.add(Bidirectional(LSTM(128,dropout=0.2,recurrent_dropout=0.2)))\n",
        "model.add(Dense(1, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc','mae','mse'])\n",
        "\n",
        "# Early Stopping\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, restore_best_weights = True)\n",
        "\n",
        "# callback list\n",
        "A = ModelCheckpoint(filepath='C:\\Oreo\\Desktop\\CS485\\ml_numerai\\Project_Models\\RNN_Model', save_best_only=True, verbose=1, monitor='val_loss')\n",
        "cb_list = [A,es]\n",
        "\n",
        "history = model.fit(xTrain, yTrain, batch_size=128,epochs=100,verbose=1,callbacks=cb_list,validation_data=(validationDataX,validationDataY),validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    C:\\Users\\Oreo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:855 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Oreo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Oreo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Oreo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2825 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Oreo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3600 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Oreo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:838 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\Oreo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\Oreo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\Oreo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:251 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_28 is incompatible with the layer: expected axis -1 of input shape to have value 310 but received input with shape (None, 310, 2)\n",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-116-295d157ba74e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mae'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingDataX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainingDataY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidationDataX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidationDataY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 872\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    873\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 916\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    917\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    751\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 753\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    754\u001b[0m             *args, **kwds))\n\u001b[0;32m    755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3049\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3050\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3051\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3444\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3279\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    660\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 662\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    663\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\Oreo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:855 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Oreo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Oreo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Oreo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2825 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Oreo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3600 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Oreo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:838 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\Oreo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\Oreo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\Oreo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:251 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_28 is incompatible with the layer: expected axis -1 of input shape to have value 310 but received input with shape (None, 310, 2)\n"
          ]
        }
      ],
      "source": [
        "# Define Model \n",
        "# Linear Regression\n",
        "def build_model_linear():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(400, kernel_initializer='normal', activation='relu', input_shape=(xtrain.shape[1],)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(.02))\n",
        "  model.add(Dense(400, kernel_initializer='normal', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(.02))\n",
        "  model.add(Dense(400, kernel_initializer='normal', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(.02))\n",
        "  model.add(Dense(400, kernel_initializer='normal', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(.02))\n",
        "  model.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
        "  return model\n",
        "\n",
        "def build_model_classification():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(128, kernel_initializer='normal', activation='relu', input_shape=(trainingDataX.shape[1],)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(.02))\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='normal'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(.02))\n",
        "  model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(.02))\n",
        "  model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(.02))\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='normal'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(.02))\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='normal'))\n",
        "  model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
        "  return model\n",
        "\n",
        "# Optimizer \n",
        "optimizer = Adam(lr=0.02)\n",
        "\n",
        "# Early Stopping\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10,restore_best_weights = True)\n",
        "\n",
        "# Build Model\n",
        "model = build_model_classification()\n",
        "\n",
        "# callback list\n",
        "A = ModelCheckpoint(filepath='C:\\Oreo\\Desktop\\CS485\\ml_numerai\\Project_Models\\Linear_Model', save_best_only=True, verbose=1, monitor='val_loss')\n",
        "cb_list = [A,es]\n",
        "\n",
        "# Train the model\n",
        "model.compile(optimizer=optimizer , loss='binary_crossentropy', metrics=['mae','mse','acc'])\n",
        "history = model.fit(trainingDataX,trainingDataY, batch_size=128,epochs=100,verbose=1,callbacks=cb_list,validation_data=(validationDataX,validationDataY),validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recurrent Neural Network With LSTMs\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(trainingDataX.shape[1], trainingDataX.shape[2]), return_sequences=True,dropout=0.2,recurrent_dropout=0.2))\n",
        "\n",
        "model.add(Bidirectional(LSTM(128,return_sequences=True,dropout=0.2,recurrent_dropout=0.2)))\n",
        "model.add(Bidirectional(LSTM(128,return_sequences=True,dropout=0.2,recurrent_dropout=0.2)))\n",
        "model.add(Bidirectional(LSTM(128,dropout=0.2,recurrent_dropout=0.2)))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "# callback list\n",
        "A = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "B = ModelCheckpoint(filepath = '/content/drive/MyDrive/Colab Notebooks/HW2/', save_best_only = True, verbose = 1)\n",
        "C = EarlyStopping(monitor='val_loss', mode='min')\n",
        "cb_list = [A, B, C]\n",
        "\n",
        "# train model\n",
        "history = model.fit(x_train, y_train, epochs=40, batch_size=200, validation_data=(x_val, y_val), callbacks = cb_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate model with 1000 decision trees\n",
        "rf = RandomForestRegressor(n_estimators = 1000, verbose=1, random_state=42, n_jobs=-2)\n",
        "\n",
        "# Train the model on training data\n",
        "rf.fit(xTrain, yTrain.flatten())"
      ]
    },
    {
      "source": [
        "# 7. Validation"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#validationDataX = validationData[feature_list]\n",
        "validationDataX = to_categorical(validationData[validationData.columns[validationData.columns.str.startswith('feature')]])\n",
        "\n",
        "validationDataY = validationData[validationData.columns[validationData.columns.str.startswith('target')]]\n",
        "\n",
        "validationDataX = np.array(validationDataX)\n",
        "validationDataX = validationDataX.reshape(-1, 1, 310)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = load_model('C:\\Oreo\\Desktop\\CS485\\ml_numerai\\Project_Models\\RNN_Model')\n",
        "\n",
        "predictions = model.predict(validationDataX)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "validationDataY.values.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cSyUnlmmG1Z"
      },
      "source": [
        "# 8. Generate Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance metrics\n",
        "errors = abs(predictions - validationDataY)\n",
        "print('Metrics for Random Forest Trained on Expanded Data')\n",
        "print('Average absolute error:', round(np.mean(errors), 2), 'degrees.')\n",
        "# Calculate mean absolute percentage error (MAPE)\n",
        "mape = np.mean(100 * (errors / test_labels))\n",
        "# Compare to baseline\n",
        "improvement_baseline = 100 * abs(mape - baseline_mape) / baseline_mape\n",
        "print('Improvement over baseline:', round(improvement_baseline, 2), '%.')\n",
        "# Calculate and display accuracy\n",
        "accuracy = 100 - mape\n",
        "print('Accuracy:', round(accuracy, 2), '%.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# predictions must have an `id` column and a `prediction_kazutsugi` column\n",
        "predictions_df = tournamentData[\"id\"].to_frame()\n",
        "predictions_df[\"prediction_kazutsugi\"] = predictions\n",
        "predictions_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload your predictions\n",
        "predictions_df.to_csv(\"predictions.csv\", index=False)\n",
        "submission_id = napi.upload_predictions(\"predictions.csv\", model_id=model_id)"
      ]
    },
    {
      "source": [
        "# 9. Works Cited\n",
        "- https://tit-btcqash.medium.com/a-comprehensive-guide-to-competing-at-numerai-70b356edbe07\n",
        "- https://towardsdatascience.com/random-forest-in-python-24d0893d51c0\n",
        "- https://realpython.com/python-timer/\n",
        "- https://www.kaggle.com/carlolepelaars/how-to-get-started-with-numerai\n",
        "- https://medium.com/machine-learning-in-practice/cheat-sheet-of-machine-learning-and-python-and-math-cheat-sheets-a4afe4e791b6\n",
        "- https://towardsdatascience.com/a-guide-to-the-hardest-data-science-tournament-on-the-planet-748f46e83690\n",
        "- https://towardsdatascience.com/improving-random-forest-in-python-part-1-893916666cd\n",
        "- https://www.geeksforgeeks.org/python-flatten-a-2d-numpy-array-into-1d-array/\n",
        "- https://docs.numer.ai/tournament/learn\n",
        "- https://forum.numer.ai/t/advice-from-the-kaggle-which-ive-found-very-useful/300\n",
        "- https://forum.numer.ai/t/model-diagnostics-feature-exposure/899\n",
        "- https://towardsdatascience.com/data-correlation-can-make-or-break-your-machine-learning-project-82ee11039cc9\n",
        "- https://www.geeksforgeeks.org/python-sort-list-of-list-by-specified-index/\n",
        "- "
      ],
      "cell_type": "markdown",
      "metadata": {}
    }
  ]
}